{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce558a-6453-4d71-9f20-298fbd1b6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Concatenate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c961b-fd3f-4257-98d3-ce6b600f1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, name='lstm_conf'):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(name)\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix', size=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=20)\n",
    "    plt.yticks(tick_marks, classes, fontsize=20)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)\n",
    "    plt.savefig(f'{name}.png')\n",
    "\n",
    "\n",
    "def multi_modal_seperate_network(output_shape, input_shape_pose, input_shape_lip):\n",
    "\n",
    "    hand_pose_input = Input(shape=input_shape_pose)\n",
    "    hand_pose_lstm = LSTM(64, return_sequences=True)(hand_pose_input)\n",
    "    hand_pose_lstm = LSTM(64)(hand_pose_lstm)\n",
    "    hand_pose_lstm = Dropout(0.5)(hand_pose_lstm)\n",
    "    \n",
    "    # Lip sub-network\n",
    "    lip_input = Input(shape=input_shape_lip)\n",
    "    lip_lstm = LSTM(64, return_sequences=True)(lip_input)\n",
    "    lip_lstm = LSTM(64)(lip_lstm)\n",
    "    lip_lstm = Dropout(0.5)(lip_lstm)\n",
    "    \n",
    "    # Combine the outputs\n",
    "    combined = Concatenate()([hand_pose_lstm, lip_lstm])\n",
    "    \n",
    "    # Final classification layer\n",
    "    output = Dense(output_shape, activation='softmax')(combined)\n",
    "    \n",
    "    # Define and compile the model\n",
    "    model = Model(inputs=[hand_pose_input, lip_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acff6f5f-0362-4efb-b158-f8131b970f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed462572-c0cc-4f5d-b4a3-e223b90dedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'NEW_MANUAL'\n",
    "# print(os.listdir(folder))\n",
    "\n",
    "npys = dict()\n",
    "for fol_singer in os.listdir(folder):\n",
    "    for fol_sign in os.listdir(f'{folder}//{fol_singer}'):\n",
    "        if fol_sign != 'P2':\n",
    "            if os.path.isdir(f'{folder}//{fol_singer}//{fol_sign}'):\n",
    "                for file in glob.glob(f'{folder}//{fol_singer}//{fol_sign}//*.npy'):\n",
    "                    np_arr = np.load(file)\n",
    "                    if fol_sign in npys.keys():\n",
    "                        npys[fol_sign].append(np_arr)\n",
    "                    else:\n",
    "                        npys[fol_sign] = [np_arr]\n",
    "\n",
    "\n",
    "data = npys.copy() # pickle.load(open('coorrds_2.pkl', 'rb'))\n",
    "data_labels = {v: k for k, v in enumerate(list(data.keys()))}\n",
    "\n",
    "print('len: ', len(data.keys()))\n",
    "print('Classes', list(data.keys()))\n",
    "\n",
    "\n",
    "info = {}\n",
    "# number of frame for process x-10:x+50\n",
    "c, b = 10, 50\n",
    "# lip coords\n",
    "lip_indexes = [0, 13, 14, 17, 37, 39, 40, 61, 78, 80, 81, 82, 84, 87, \\\n",
    "               88, 91, 95, 146, 178, 181, 185, 191, 267, 269, 270, 291, \\\n",
    "               308, 310, 311, 312, 314, 317, 318, 321, 324, 375, 402, 405, \\\n",
    "               409, 415]\n",
    "\n",
    "general_data = {}\n",
    "general_lip_data = {}\n",
    "\n",
    "for let in data.keys():\n",
    "    let_sequence = []\n",
    "    let_lip_sequence = []\n",
    "    for j in range(len(data[let])):\n",
    "        sequence = []\n",
    "        lip_sequence = []\n",
    "        for i in range(data[let][j].shape[0]):\n",
    "            if np.any(data[let][j][i][-21*3*2:]):\n",
    "                # print(i)\n",
    "                break\n",
    "        if i-c > -1:\n",
    "            for ret in data[let][j][i-c:i+b]:\n",
    "                sequence.append(np.append(ret[:132], ret[-126:]))\n",
    "                \n",
    "                face_landmarks = ret[33*4:-(21*3*2)].reshape(468, 3)\n",
    "                lip_coords = []\n",
    "                for idx in lip_indexes:\n",
    "                    lip_coords.append((face_landmarks[idx]))\n",
    "                lip_coords = np.array(lip_coords).flatten()\n",
    "                lip_sequence.append(lip_coords)\n",
    "            # for i in range(data[let][j].shape[0]):\n",
    "            #     # if np.any(data[let][j][i,-126:]):\n",
    "            #     sequence.append(np.append(data[let][j][i,:132], data[let][j][i,-126:]))\n",
    "            let_sequence.append(np.array(sequence))\n",
    "            let_lip_sequence.append(np.array(lip_sequence))\n",
    "            a = let_sequence[-1].shape\n",
    "            if a in info.keys():\n",
    "                info[a] +=1\n",
    "            else:\n",
    "                info[a] =1\n",
    "    general_data[let] = let_sequence\n",
    "    general_lip_data[let] = let_lip_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133297b-f33d-4a71-a3f6-2cad2625ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "final_label = []\n",
    "\n",
    "final_data_lip = []\n",
    "final_label_lip = []\n",
    "\n",
    "for let in general_data.keys():\n",
    "    for obj in general_data[let]:\n",
    "        # if obj.shape[0] >= 30 and obj.shape[0] <= 45:\n",
    "        if obj.shape[0] >= c+b:\n",
    "            final_data.append(obj[:c+b])\n",
    "            final_label.append(let)\n",
    "    for obj in general_lip_data[let]:\n",
    "        # if obj.shape[0] >= 30 and obj.shape[0] <= 45:\n",
    "        if obj.shape[0] >= c+b:\n",
    "            final_data_lip.append(obj[:c+b])\n",
    "            final_label_lip.append(let)\n",
    "\n",
    "final_label = np.array([data_labels[i] for i in final_label])\n",
    "final_data = np.array(final_data)\n",
    "\n",
    "final_label_lip = np.array([data_labels[i] for i in final_label_lip])\n",
    "final_data_lip = np.array(final_data_lip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a467ae-950f-4d2d-9a1e-6542fff1a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = y_test.shape[1]\n",
    "input_shape_pose = x_test.shape[1:]\n",
    "input_shape_lip = x_test_lip.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606c226-606d-4d29-92f3-c3db2087495c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df10990-272b-4bf9-9f00-640ce3b21f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e36a7-3c70-4a6c-b992-b9582044aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    model = multi_modal_seperate_network(output_shape, input_shape_pose, input_shape_lip)\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    history = model.fit([x_train, x_train_lip], y_train, \n",
    "              epochs=1000, \n",
    "              batch_size=32, \n",
    "              validation_split=0.1,\n",
    "              callbacks=[EarlyStopping(patience=100)])\n",
    "    test_loss, test_acc = model.evaluate([x_test, x_test_lip], y_test)\n",
    "    print(f'Test accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
